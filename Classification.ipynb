{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"notebook\")\n",
    "#sns.set_context(\"poster\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Basic Classification Algorithms</h1>\n",
    "\n",
    "Here we review six of the most well-known classification algorithms. \n",
    "\n",
    "Two linear:\n",
    "\n",
    "<ul>\n",
    "    <li>Logistic Regression.</li>\n",
    "    <li>Linear Discriminant Analysis.</li>\n",
    "</ul>\n",
    "and four non-linear:\n",
    "<ul>\n",
    "    <li>k-nn - k-Nearest Neighbors.</li>\n",
    "    <li>Naive Bayes.</li>\n",
    "    <li>CART - Classification and Regression Trees.</li>\n",
    "    <li>SVM - Support Vector Machines.</li>\n",
    "</ul>\n",
    "\n",
    "Then we will address the simple and common question of <b><i>What algorithms should I use in this dataset?</b></i>\n",
    "\n",
    "In all cases we will use a dataset that we are familiar with, the Pima Indians dataset, with a 10-fold cross-validation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Pima_indians_cowboy_1889.jpg\">\n",
    "\n",
    "In this exercise we will use one of the traditional Machine Learning dataset, the Pima Indians diabetes dataset.\n",
    "\n",
    "This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n",
    "\n",
    "Content\n",
    "The datasets consists of several medical predictor variables and one target variable, <b>Outcome</b>. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n",
    "<blockquote>\n",
    "        <ul style=\"list-style-type:square;\">\n",
    "            <li>Pregnancies</li> \n",
    "            <li>Glucose</li>\n",
    "            <li>BloodPressure</li>\n",
    "            <li>SkinThickness</li>\n",
    "            <li>Insulin</li>\n",
    "            <li>BMI</li>\n",
    "            <li>DiabetesPedigreeFunction (scores de likelihood of diabetes based on family history)</li>\n",
    "            <li>Age</li>\n",
    "            <li>Outcome</li>\n",
    "        </ul>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Pima indians dataset and separate input and output components \n",
    "\n",
    "from numpy import set_printoptions\n",
    "set_printoptions(precision=3)\n",
    "\n",
    "filename=\"pima-indians-diabetes.data.csv\"\n",
    "names=[\"pregnancies\", \"glucose\", \"pressure\", \"skin\", \"insulin\", \"bmi\", \"pedi\", \"age\", \"outcome\"]\n",
    "p_indians=pd.read_csv(filename, names=names)\n",
    "p_indians.head()\n",
    "\n",
    "# First we separate into input and output components\n",
    "array=p_indians.values\n",
    "X=array[:,0:8]\n",
    "y=array[:,8]\n",
    "np.set_printoptions(suppress=True)\n",
    "X\n",
    "pd.DataFrame(X).head()\n",
    "\n",
    "#Now we standarize our data \n",
    "\n",
    "std_scaler=preprocessing.StandardScaler()\n",
    "X_std=std_scaler.fit_transform(X)\n",
    "\n",
    "minmax_scaler=preprocessing.MinMaxScaler()\n",
    "X_minmax=minmax_scaler.fit_transform(X)\n",
    "\n",
    "# Create the DataFrames for plotting\n",
    "resall=pd.DataFrame()\n",
    "res_w1=pd.DataFrame()\n",
    "res_w2=pd.DataFrame()\n",
    "res_w3=pd.DataFrame()\n",
    "# creando df vacíos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "It is probably the best known and the oldest. We are also pretty familiar with it !\n",
    "\n",
    "Logistic regression assumes a Gaussian distribution for the numeric input variables and can solve binary and multi-class classification problems. \n",
    "\n",
    "We will use the <b>LogisticRegression</b> class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "kfold=KFold(n_splits=10, random_state=7)\n",
    "\n",
    "model=LogisticRegression(solver=\"liblinear\")\n",
    "\n",
    "results=cross_val_score(model, X, y, cv=kfold)\n",
    "\n",
    "print(f'Logistic Regression - Accuracy {results.mean()*100:.3f}% std {results.std()*100:3f}')\n",
    "\n",
    "results_scl=cross_val_score(model, X_std, y, cv=kfold)\n",
    "\n",
    "print(f'Logistic Regression (-1..1) - Accuracy {results_scl.mean()*100:.3f}% std {results_scl.std()*100:3f}')\n",
    "\n",
    "results_minmax=cross_val_score(model, X_minmax, y, cv=kfold)\n",
    "\n",
    "print(f'Logistic Regression ( 0..1) - Accuracy {results_minmax.mean()*100:.3f}% std {results_minmax.std()*100:3f}')\n",
    "\n",
    "# if the range of variables is large scaling doesn't matter in a log regression \n",
    "# but if you are not sure if they are (or you don't want to check ... ) just try ! \n",
    "\n",
    "res_w1[\"Res\"]=results\n",
    "res_w1[\"Type\"]=\"log\"\n",
    "\n",
    "res_w2[\"Res\"]=results_scl\n",
    "res_w2[\"Type\"]=\"log -1..1\"\n",
    "\n",
    "res_w3[\"Res\"]=results_minmax\n",
    "res_w3[\"Type\"]=\"log 0..1\"\n",
    "\n",
    "resall=pd.concat([resall,res_w1,res_w2,res_w3], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA - Linear Discriminant Analysis\n",
    "\n",
    "Linear Discriminant Analysis or discriminant analysis is a generalization of Fisher's linear discriminant, originally developed by Ronald Fisher in 1936. Although it is different from ANOVA (Analysis of variance), they are closely related. \n",
    "\n",
    "LDA also assumes a Gaussian distribution of the numerical input variables and can be used for binary or multi-class classification. \n",
    "\n",
    "We will use the <b>LinearDiscriminantAnalysis</b> class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA - Linear Discriminant Analysis \n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "kfold=KFold(n_splits=10, random_state=7)\n",
    "\n",
    "model=LinearDiscriminantAnalysis()\n",
    "\n",
    "results=cross_val_score(model, X, y, cv=kfold)\n",
    "\n",
    "print(f'LDA Linear Discriminant Analysis - Accuracy {results.mean()*100:.3f}% std {results.std()*100:3f}')\n",
    "\n",
    "results_scl=cross_val_score(model, X_std, y, cv=kfold)\n",
    "\n",
    "print(f'LDA (-1..1) - Accuracy {results_scl.mean()*100:.3f}% std {results_scl.std()*100:3f}')\n",
    "\n",
    "results_minmax=cross_val_score(model, X_minmax, y, cv=kfold)\n",
    "\n",
    "print(f'LDA ( 0..1) - Accuracy {results_minmax.mean()*100:.3f}% std {results_minmax.std()*100:3f}')\n",
    "\n",
    "\n",
    "res_w1[\"Res\"]=results\n",
    "res_w1[\"Type\"]=\"LDA\"\n",
    "\n",
    "res_w2[\"Res\"]=results_scl\n",
    "res_w2[\"Type\"]=\"LDA -1..1\"\n",
    "\n",
    "res_w3[\"Res\"]=results_minmax\n",
    "res_w3[\"Type\"]=\"LDA 0..1\"\n",
    "\n",
    "resall=pd.concat([resall,res_w1,res_w2,res_w3], ignore_index=True)\n",
    "\n",
    "\n",
    "# Accuracies me salen iguales, porque como primero busca VAPS y VEPS, cualquier cambio lineal me dará lo mismo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-nn k-Nearest Neighbors\n",
    "\n",
    "k-Nearerst Neighbors is a non-linear machine learning algorithm that uses distance metrics to find the most similar k-elements, taking the meand outcome of the neighbors as the prediction.\n",
    "\n",
    "One interesting advantage of this algorithm is that we can choose a different metric for calculating the distance. The default metric is Minkowski, equivalent to euclidean (with p=2). It can be easily transformed to Mnahattan distance with p=1. \n",
    "\n",
    "For constructing a knn model you must use the <b>KNeighorsClassifier</b> class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Classification\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "kfold=KFold(n_splits=10, random_state=7)\n",
    "\n",
    "model=KNeighborsClassifier()\n",
    "\n",
    "results=cross_val_score(model, X, y, cv=kfold)\n",
    "\n",
    "print(f'KNN - Accuracy {results.mean()*100:.3f}% std {results.std()*100:3f}')\n",
    "\n",
    "results_scl=cross_val_score(model, X_std, y, cv=kfold)\n",
    "\n",
    "print(f'KNN (-1..1) - Accuracy {results_scl.mean()*100:.3f}% std {results_scl.std()*100:3f}')\n",
    "\n",
    "results_minmax=cross_val_score(model, X_minmax, y, cv=kfold)\n",
    "\n",
    "print(f'KNN ( 0..1) - Accuracy {results_minmax.mean()*100:.3f}% std {results_minmax.std()*100:3f}')\n",
    "\n",
    "# scaling in knn is necessary ...\n",
    "\n",
    "res_w1[\"Res\"]=results\n",
    "res_w1[\"Type\"]=\"KNN\"\n",
    "\n",
    "res_w2[\"Res\"]=results_scl\n",
    "res_w2[\"Type\"]=\"KNN -1..1\"\n",
    "\n",
    "res_w3[\"Res\"]=results_minmax\n",
    "res_w3[\"Type\"]=\"KNN 0..1\"\n",
    "\n",
    "resall=pd.concat([resall,res_w1,res_w2,res_w3], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "In Naive Bayes class labels are represented by a vector of features and each feature is considered independent of the others (the naive part of the name comes from this assumption). Probabilities are calculated following the bayesian approach. \n",
    "\n",
    "In spite of its oversimplified assumptions, the algorithm works quite well in complex, real world situations. The algorithm is particularly usefull with small samples of data. \n",
    "\n",
    "For Naive Bayes we will use the <b>GaussianNB</b> class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "kfold=KFold(n_splits=10, random_state=7)\n",
    "\n",
    "model=GaussianNB()\n",
    "\n",
    "results=cross_val_score(model, X, y, cv=kfold)\n",
    "\n",
    "print(f'Naive Bayes - Accuracy {results.mean()*100:.3f}% std {results.std()*100:3f}')\n",
    "\n",
    "results_scl=cross_val_score(model, X_std, y, cv=kfold)\n",
    "\n",
    "print(f'Naive Bayes (-1..1) - Accuracy {results_scl.mean()*100:.3f}% std {results_scl.std()*100:3f}')\n",
    "\n",
    "results_minmax=cross_val_score(model, X_minmax, y, cv=kfold)\n",
    "\n",
    "print(f'Naive Bayes ( 0..1) - Accuracy {results_minmax.mean()*100:.3f}% std {results_minmax.std()*100:3f}')\n",
    "\n",
    "res_w1[\"Res\"]=results\n",
    "res_w1[\"Type\"]=\"NB\"\n",
    "\n",
    "res_w2[\"Res\"]=results_scl\n",
    "res_w2[\"Type\"]=\"NB -1..1\"\n",
    "\n",
    "res_w3[\"Res\"]=results_minmax\n",
    "res_w3[\"Type\"]=\"NB 0..1\"\n",
    "\n",
    "resall=pd.concat([resall,res_w1,res_w2,res_w3], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CART - Classification and Regression Trees\n",
    "\n",
    "Cart builds a binary tree from the data where the splits are chosen greedly evaluating all the attributes in order to minimize a cost function (Gini index or entropy typically).\n",
    "\n",
    "They are the base for random forests and more sophisticated algorithms. \n",
    "\n",
    "For CART we will use the <b>DecisionTreeClassifier</b> class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Trees\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "seed=7\n",
    "\n",
    "kfold=KFold(n_splits=10, random_state=seed)\n",
    "\n",
    "model=DecisionTreeClassifier(class_weight=\"balanced\", random_state=seed)\n",
    "\n",
    "\n",
    "results=cross_val_score(model, X, y, cv=kfold)\n",
    "\n",
    "print(f'Decision Tree - Accuracy {results.mean()*100:.3f}% std {results.std()*100:3f}')\n",
    "\n",
    "results_scl=cross_val_score(model, X_std, y, cv=kfold)\n",
    "\n",
    "print(f'Decision Tree (-1..1) - Accuracy {results_scl.mean()*100:.3f}% std {results_scl.std()*100:3f}')\n",
    "\n",
    "results_minmax=cross_val_score(model, X_minmax, y, cv=kfold)\n",
    "\n",
    "print(f'Decision Tree ( 0..1) - Accuracy {results_minmax.mean()*100:.3f}% std {results_minmax.std()*100:3f}')\n",
    "\n",
    "res_w1[\"Res\"]=results\n",
    "res_w1[\"Type\"]=\"DT\"\n",
    "\n",
    "res_w2[\"Res\"]=results_scl\n",
    "res_w2[\"Type\"]=\"DT -1..1\"\n",
    "\n",
    "res_w3[\"Res\"]=results_minmax\n",
    "res_w3[\"Type\"]=\"DT 0..1\"\n",
    "\n",
    "resall=pd.concat([resall,res_w1,res_w2,res_w3], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Displaying a tree\n",
    "#    you need to install graphviz\n",
    "# ! pip install graphviz\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "from sklearn import tree\n",
    "from graphviz import Source\n",
    "from IPython.display import SVG, display\n",
    "from ipywidgets import interactive\n",
    "\n",
    "\n",
    "style = \"<style>svg{width:70% !important;height:70% !important;}</style>\"\n",
    "HTML(style)\n",
    "\n",
    "model=DecisionTreeClassifier(class_weight=\"balanced\", random_state=seed)\n",
    "model.fit(X,y)\n",
    "\n",
    "graph=Source(tree.export_graphviz(model,\n",
    "        out_file=None,      \n",
    "        feature_names=p_indians.columns[0:-1],\n",
    "        class_names=['No Diabetes','Diabetes'],\n",
    "        filled=True,\n",
    "        rounded=True))\n",
    "\n",
    "display(SVG(graph.pipe(format=\"svg\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Users/GEORGINA/Anaconda3/envs/keras/Library/bin/graphviz/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Displaying a tree\n",
    "#    you need to install graphviz\n",
    "# ! pip install graphviz\n",
    "\n",
    "from IPython.display import HTML # te pone el jupyter notebook en formato html\n",
    "\n",
    "from sklearn import tree\n",
    "from graphviz import Source\n",
    "from IPython.display import SVG, display\n",
    "from ipywidgets import interactive\n",
    "\n",
    "seed=7\n",
    "\n",
    "def plot_tree(crit, split, depth, min_split, min_leaf=1):\n",
    "    \n",
    "    indians_tree=DecisionTreeClassifier(random_state=seed,\n",
    "                criterion=crit, # la función que mide calidad de un split, coeficiente de Gini. (aunque tb entropía)\n",
    "                splitter=split,\n",
    "                max_depth=depth,\n",
    "                min_samples_split=min_split, # el mín. numero de casos que debe tener un split para que se haga (ej.2)\n",
    "                min_samples_leaf=min_leaf)\n",
    "    indians_tree.fit(X,y)\n",
    "    \n",
    "    graph=Source(tree.export_graphviz(indians_tree,\n",
    "            out_file=None,\n",
    "            feature_names=p_indians.columns[0:-1],\n",
    "            class_names=[\"0\",\"1\",\"2\"],\n",
    "            filled=True,\n",
    "            rounded=True))\n",
    "    display(SVG(graph.pipe(format=\"svg\"))) # seria el equivalente al 'show'\n",
    "    \n",
    "    return indians_tree\n",
    "\n",
    "inter=interactive(plot_tree,\n",
    "        crit=[\"gini\",\"entropy\"],\n",
    "        split=[\"best\",\"random\"],\n",
    "        depth=[None,1,2,3,4],\n",
    "        min_split=(2,100),\n",
    "        min_leaf=(1,200))\n",
    "\n",
    "display(inter)\n",
    "\n",
    "# Gini y Entropía para calcular desigualdad. gini, maximiazr desigualdad. entropia, parámetro que te crea mayor distorsión/vari\n",
    "\n",
    "# Interactive me permite poner las distintas opciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "Support vector machines seeks a line that separates best two classes. The data instances that are closest to this line are, better separating the classes, are called support vectors. \n",
    "\n",
    "Support Vector Machines have the advantage that you can change the kernel function to use. Radial basis function is used by default, a pretty powerful one. \n",
    "\n",
    "You can construct a SVM model with the <b>SVC</b> class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM - Support Vector Machines\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "kfold=KFold(n_splits=10, random_state=7)\n",
    "\n",
    "model=SVC(gamma=\"scale\")\n",
    "\n",
    "results=cross_val_score(model, X, y, cv=kfold)\n",
    "\n",
    "print(f'Support Vector Machines - Accuracy {results.mean()*100:.3f}% std {results.std()*100:3f}')\n",
    "\n",
    "results_scl=cross_val_score(model, X_std, y, cv=kfold)\n",
    "\n",
    "print(f'SVM (-1..1) - Accuracy {results_scl.mean()*100:.3f}% std {results_scl.std()*100:3f}')\n",
    "\n",
    "results_minmax=cross_val_score(model, X_minmax, y, cv=kfold)\n",
    "\n",
    "print(f'SVM ( 0..1) - Accuracy {results_minmax.mean()*100:.3f}% std {results_minmax.std()*100:3f}')\n",
    "\n",
    "# the importance of scaling depends on the kernel used\n",
    "\n",
    "res_w1[\"Res\"]=results\n",
    "res_w1[\"Type\"]=\"SVM\"\n",
    "\n",
    "res_w2[\"Res\"]=results_scl\n",
    "res_w2[\"Type\"]=\"SVM -1..1\"\n",
    "\n",
    "res_w3[\"Res\"]=results_minmax\n",
    "res_w3[\"Type\"]=\"SVM 0..1\"\n",
    "\n",
    "resall=pd.concat([resall,res_w1,res_w2,res_w3], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's compare them all \n",
    "\n",
    "plt.figure(figsize=(15,9))\n",
    "\n",
    "sns.boxplot(data=resall, x=\"Type\", y=\"Res\")\n",
    "\n",
    "sns.swarmplot(data=resall, x=\"Type\", y=\"Res\", color=\"royalblue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color=\"red\" size=6>Mission 1</font>\n",
    "\n",
    "a) Do the same with the Times Ranking predicting to be among the 10 best Business Schools.<br><br>\n",
    "b) Try the Titanic dataset (you'll find all the info that you need in Kaggle). \n",
    "<br><br>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) Do the same with the Times Ranking predicting to be among the 10 best Business Schools.\n",
    "\n",
    "# Import dataset and do corresponding adjustments (just copy-pasted from my 'Feature Selection' Notebook)\n",
    "times = pd.read_csv('timesData.csv')\n",
    "times.world_rank.replace('=.','.', regex=True, inplace=True)\n",
    "times['world_rank'] = times['world_rank'].str.split('-').str.get(0).astype(float)\n",
    "times.world_rank = pd.to_numeric(times.world_rank, errors='coerce')\n",
    "times.dropna(subset=['world_rank'], axis=0, inplace=True) # DROPNA only of 'world_rank'\n",
    "times.international = pd.to_numeric(times.international, errors='coerce')\n",
    "times.income = pd.to_numeric(times.income, errors='coerce')\n",
    "times.num_students.replace('\\D','', regex=True, inplace=True)\n",
    "times.num_students = pd.to_numeric(times.num_students, errors='coerce')\n",
    "times.international_students.replace('\\D','', regex=True, inplace=True)\n",
    "times.international_students = pd.to_numeric(times.international_students, errors='coerce')\n",
    "times.international_students = times.international_students/100\n",
    "times['females'] = times.female_male_ratio.str.split(':').str.get(0) # manipulate so I can use the data of this column.\n",
    "times['males'] = times.female_male_ratio.str.split(':').str.get(1)\n",
    "times.females = pd.to_numeric(times.females, errors='coerce')\n",
    "times.males = pd.to_numeric(times.males, errors='coerce')\n",
    "times['ratio_male_to_female'] = times.males/times.females\n",
    "times.drop(columns='total_score',inplace=True)\n",
    "times.dropna(inplace=True)\n",
    "times.head()\n",
    "\n",
    "# Select variables and transform to numpy arrays\n",
    "X = pd.concat([times.iloc[:,3:11],times['ratio_male_to_female']],axis=1).values\n",
    "times['Top10'] = times['world_rank']<11 # TOP 10\n",
    "y = times['Top10'].values # ¡vigilar mayúsculas y minúsculas! (debido a esto me daba error)\n",
    "\n",
    "# Another way of defining the 'y' in this case, would be with a binarizer this way:\n",
    "    # Y = times.iloc[:,0:1].values\n",
    "    # new_pbinarizer=Binarizer(threshold=10).fit(Y) # TOP 10\n",
    "    # times['new_pbinaryY']=new_pbinarizer.transform(Y)\n",
    "    # times['new_new_pbinaryY'] = times['new_pbinaryY'].apply(lambda x: np.abs(x - 1))\n",
    "    # Y = times['new_new_pbinaryY']\n",
    "\n",
    "\n",
    "# Now we standarize our data (taking the data from above, since the questions asks to do the 'same' and variable names coincide)\n",
    "\n",
    "std_scaler=preprocessing.StandardScaler()\n",
    "X_std=std_scaler.fit_transform(X)\n",
    "\n",
    "minmax_scaler=preprocessing.MinMaxScaler()\n",
    "X_minmax=minmax_scaler.fit_transform(X)\n",
    "\n",
    "# Create the DataFrames for plotting (right now empty, but will be filled later)\n",
    "resall=pd.DataFrame()\n",
    "res_w1=pd.DataFrame()\n",
    "res_w2=pd.DataFrame()\n",
    "res_w3=pd.DataFrame()\n",
    "\n",
    "# And now I proceed with the different algorithms, applying each one to the three different (scaled)'version' of my variables.\n",
    "\n",
    "print()\n",
    "print(\"---------------------------------------- LOGISTIC REGRESSION ------------------------------------------------\")\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "# Imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# KFold\n",
    "kfold=KFold(n_splits=10, random_state=7)\n",
    "\n",
    "model=LogisticRegression(solver=\"liblinear\")\n",
    "\n",
    "# Accuracy for X\n",
    "results=cross_val_score(model, X, y, cv=kfold)\n",
    "print(f'Logistic Regression - Accuracy {results.mean()*100:.3f}% std {results.std()*100:3f}')\n",
    "\n",
    "# Accuracy for X_std\n",
    "results_scl=cross_val_score(model, X_std, y, cv=kfold)\n",
    "print(f'Logistic Regression (-1..1) - Accuracy {results_scl.mean()*100:.3f}% std {results_scl.std()*100:3f}')\n",
    "\n",
    "# Accuracy for X_minmax\n",
    "results_minmax=cross_val_score(model, X_minmax, y, cv=kfold)\n",
    "print(f'Logistic Regression ( 0..1) - Accuracy {results_minmax.mean()*100:.3f}% std {results_minmax.std()*100:3f}')\n",
    "\n",
    "\n",
    "res_w1[\"Res\"]=results\n",
    "res_w1[\"Type\"]=\"log\"\n",
    "\n",
    "res_w2[\"Res\"]=results_scl\n",
    "res_w2[\"Type\"]=\"log -1..1\"\n",
    "\n",
    "res_w3[\"Res\"]=results_minmax\n",
    "res_w3[\"Type\"]=\"log 0..1\"\n",
    "\n",
    "resall=pd.concat([resall,res_w1,res_w2,res_w3], ignore_index=True)\n",
    "\n",
    "print()\n",
    "print(\"---------------------------------------- LDA ------------------------------------------------\")\n",
    "\n",
    "# LDA - Linear Discriminant Analysis \n",
    "\n",
    "# Imports (no need for KFold and so on because they are above)\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# KFold\n",
    "kfold=KFold(n_splits=10, random_state=7)\n",
    "\n",
    "model=LinearDiscriminantAnalysis()\n",
    "\n",
    "results=cross_val_score(model, X, y, cv=kfold)\n",
    "\n",
    "print(f'LDA Linear Discriminant Analysis - Accuracy {results.mean()*100:.3f}% std {results.std()*100:3f}')\n",
    "\n",
    "results_scl=cross_val_score(model, X_std, y, cv=kfold)\n",
    "\n",
    "print(f'LDA (-1..1) - Accuracy {results_scl.mean()*100:.3f}% std {results_scl.std()*100:3f}')\n",
    "\n",
    "results_minmax=cross_val_score(model, X_minmax, y, cv=kfold)\n",
    "\n",
    "print(f'LDA ( 0..1) - Accuracy {results_minmax.mean()*100:.3f}% std {results_minmax.std()*100:3f}')\n",
    "\n",
    "\n",
    "res_w1[\"Res\"]=results\n",
    "res_w1[\"Type\"]=\"LDA\"\n",
    "\n",
    "res_w2[\"Res\"]=results_scl\n",
    "res_w2[\"Type\"]=\"LDA -1..1\"\n",
    "\n",
    "res_w3[\"Res\"]=results_minmax\n",
    "res_w3[\"Type\"]=\"LDA 0..1\"\n",
    "\n",
    "resall=pd.concat([resall,res_w1,res_w2,res_w3], ignore_index=True)\n",
    "\n",
    "print()\n",
    "print(\"---------------------------------------- KNN ------------------------------------------------\")\n",
    "# KNN Classification\n",
    "\n",
    "# Imports (no need for KFold and so on because I have imported them above in this same cell)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# KFold\n",
    "kfold=KFold(n_splits=10, random_state=7)\n",
    "model=KNeighborsClassifier()\n",
    "\n",
    "\n",
    "results=cross_val_score(model, X, y, cv=kfold)\n",
    "\n",
    "print(f'KNN - Accuracy {results.mean()*100:.3f}% std {results.std()*100:3f}')\n",
    "\n",
    "results_scl=cross_val_score(model, X_std, y, cv=kfold)\n",
    "\n",
    "print(f'KNN (-1..1) - Accuracy {results_scl.mean()*100:.3f}% std {results_scl.std()*100:3f}')\n",
    "\n",
    "results_minmax=cross_val_score(model, X_minmax, y, cv=kfold)\n",
    "\n",
    "print(f'KNN ( 0..1) - Accuracy {results_minmax.mean()*100:.3f}% std {results_minmax.std()*100:3f}')\n",
    "\n",
    "# scaling in knn is necessary ...\n",
    "\n",
    "res_w1[\"Res\"]=results\n",
    "res_w1[\"Type\"]=\"KNN\"\n",
    "\n",
    "res_w2[\"Res\"]=results_scl\n",
    "res_w2[\"Type\"]=\"KNN -1..1\"\n",
    "\n",
    "res_w3[\"Res\"]=results_minmax\n",
    "res_w3[\"Type\"]=\"KNN 0..1\"\n",
    "\n",
    "resall=pd.concat([resall,res_w1,res_w2,res_w3], ignore_index=True)\n",
    "\n",
    "print()\n",
    "print(\"---------------------------------------- NAIVE BAYES ------------------------------------------------\")\n",
    "# Naive Bayes\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "kfold=KFold(n_splits=10, random_state=7)\n",
    "\n",
    "model=GaussianNB()\n",
    "\n",
    "results=cross_val_score(model, X, y, cv=kfold)\n",
    "\n",
    "print(f'Naive Bayes - Accuracy {results.mean()*100:.3f}% std {results.std()*100:3f}')\n",
    "\n",
    "results_scl=cross_val_score(model, X_std, y, cv=kfold)\n",
    "\n",
    "print(f'Naive Bayes (-1..1) - Accuracy {results_scl.mean()*100:.3f}% std {results_scl.std()*100:3f}')\n",
    "\n",
    "results_minmax=cross_val_score(model, X_minmax, y, cv=kfold)\n",
    "\n",
    "print(f'Naive Bayes ( 0..1) - Accuracy {results_minmax.mean()*100:.3f}% std {results_minmax.std()*100:3f}')\n",
    "\n",
    "res_w1[\"Res\"]=results\n",
    "res_w1[\"Type\"]=\"NB\"\n",
    "\n",
    "res_w2[\"Res\"]=results_scl\n",
    "res_w2[\"Type\"]=\"NB -1..1\"\n",
    "\n",
    "res_w3[\"Res\"]=results_minmax\n",
    "res_w3[\"Type\"]=\"NB 0..1\"\n",
    "\n",
    "resall=pd.concat([resall,res_w1,res_w2,res_w3], ignore_index=True)\n",
    "\n",
    "print()\n",
    "print(\"---------------------------------------- DECISION TREE ------------------------------------------------\")\n",
    "\n",
    "# Decision Trees\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "seed=7\n",
    "\n",
    "kfold=KFold(n_splits=10, random_state=seed)\n",
    "\n",
    "model=DecisionTreeClassifier(class_weight=\"balanced\", random_state=seed)\n",
    "\n",
    "\n",
    "results=cross_val_score(model, X, y, cv=kfold)\n",
    "\n",
    "print(f'Decision Tree - Accuracy {results.mean()*100:.3f}% std {results.std()*100:3f}')\n",
    "\n",
    "results_scl=cross_val_score(model, X_std, y, cv=kfold)\n",
    "\n",
    "print(f'Decision Tree (-1..1) - Accuracy {results_scl.mean()*100:.3f}% std {results_scl.std()*100:3f}')\n",
    "\n",
    "results_minmax=cross_val_score(model, X_minmax, y, cv=kfold)\n",
    "\n",
    "print(f'Decision Tree ( 0..1) - Accuracy {results_minmax.mean()*100:.3f}% std {results_minmax.std()*100:3f}')\n",
    "\n",
    "res_w1[\"Res\"]=results\n",
    "res_w1[\"Type\"]=\"DT\"\n",
    "\n",
    "res_w2[\"Res\"]=results_scl\n",
    "res_w2[\"Type\"]=\"DT -1..1\"\n",
    "\n",
    "res_w3[\"Res\"]=results_minmax\n",
    "res_w3[\"Type\"]=\"DT 0..1\"\n",
    "\n",
    "resall=pd.concat([resall,res_w1,res_w2,res_w3], ignore_index=True)\n",
    "\n",
    "\n",
    "# Displaying the interactive tree\n",
    "\n",
    "# Imports\n",
    "from IPython.display import HTML \n",
    "from sklearn import tree\n",
    "from graphviz import Source\n",
    "from IPython.display import SVG, display\n",
    "from ipywidgets import interactive\n",
    "\n",
    "seed=7\n",
    "\n",
    "def plot_tree(crit, split, depth, min_split, min_leaf=1): # mejor ponerlo la próxima vez en celda aparte.\n",
    "    \n",
    "    times_tree=DecisionTreeClassifier(random_state=seed,\n",
    "                criterion=crit, \n",
    "                splitter=split,\n",
    "                max_depth=depth,\n",
    "                min_samples_split=min_split,  \n",
    "                min_samples_leaf=min_leaf)\n",
    "    times_tree.fit(X,y)\n",
    "    \n",
    "    graph=Source(tree.export_graphviz(times_tree,\n",
    "            out_file=None,\n",
    "            feature_names=times.columns[3:-5], # Lo ideal hubiera sido al principio haber dejado el df ya apañado\n",
    "            class_names=[\"0\",\"1\"], # I only have two classes: top10 or not top10\n",
    "            filled=True,\n",
    "            rounded=True))\n",
    "    display(SVG(graph.pipe(format=\"svg\"))) \n",
    "    \n",
    "    return times_tree\n",
    "\n",
    "inter=interactive(plot_tree,\n",
    "        crit=[\"gini\",\"entropy\"],\n",
    "        split=[\"best\",\"random\"],\n",
    "        depth=[None,1,2,3,4],\n",
    "        min_split=(2,100),\n",
    "        min_leaf=(1,200))\n",
    "\n",
    "display(inter)\n",
    "\n",
    "print()\n",
    "print(\"---------------------------------------- SVM ------------------------------------------------\")\n",
    "\n",
    "# SVM - Support Vector Machines\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "kfold=KFold(n_splits=10, random_state=7)\n",
    "\n",
    "model=SVC(gamma=\"scale\")\n",
    "\n",
    "results=cross_val_score(model, X, y, cv=kfold)\n",
    "\n",
    "print(f'Support Vector Machines - Accuracy {results.mean()*100:.3f}% std {results.std()*100:3f}')\n",
    "\n",
    "results_scl=cross_val_score(model, X_std, y, cv=kfold)\n",
    "\n",
    "print(f'SVM (-1..1) - Accuracy {results_scl.mean()*100:.3f}% std {results_scl.std()*100:3f}')\n",
    "\n",
    "results_minmax=cross_val_score(model, X_minmax, y, cv=kfold)\n",
    "\n",
    "print(f'SVM ( 0..1) - Accuracy {results_minmax.mean()*100:.3f}% std {results_minmax.std()*100:3f}')\n",
    "\n",
    "# the importance of scaling depends on the kernel used\n",
    "\n",
    "res_w1[\"Res\"]=results\n",
    "res_w1[\"Type\"]=\"SVM\"\n",
    "\n",
    "res_w2[\"Res\"]=results_scl\n",
    "res_w2[\"Type\"]=\"SVM -1..1\"\n",
    "\n",
    "res_w3[\"Res\"]=results_minmax\n",
    "res_w3[\"Type\"]=\"SVM 0..1\"\n",
    "\n",
    "resall=pd.concat([resall,res_w1,res_w2,res_w3], ignore_index=True)\n",
    "\n",
    "print()\n",
    "print(\"---------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"---------------------------------------- ALGORITHM COMPARISON ------------------------------------------------\")\n",
    "\n",
    "# Now let's compare them all \n",
    "\n",
    "plt.figure(figsize=(15,9))\n",
    "\n",
    "sns.boxplot(data=resall, x=\"Type\", y=\"Res\")\n",
    "\n",
    "sns.swarmplot(data=resall, x=\"Type\", y=\"Res\", color=\"royalblue\")\n",
    "\n",
    "\n",
    "\n",
    "# INTERPRETATION: (in next cell:)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the box plots above, I would discard the algorithms that present a low accuracy (the line in the middle of the box plot) and high variance (the box representing the middle 50% of the accuracy results for each of the algorithms.\n",
    "Therefore, I would totally discard in this case Naive Bayes first.\n",
    "I would select the KNN without scaling the features because not only the accuracy is high, but also there is low variance and most of the dots (the results of the accuracy) are concentrated at the top as well.\n",
    "As a second option, the SVM (having scaled the features with minmax or with the standardizer).\n",
    "And as a third valid option, the logistic regression with the minmax features.\n",
    "\n",
    "KNN y SVM funcionan con distancias euclideanas, por lo que si no haces estandarización funcionan fatal.\n",
    "LDA ya hace estandarización automáticamente, por lo que de las 3 formas sale igual.\n",
    "Naive Bayes funciona por distribuciones, no por distancias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# B) Try the Titanic dataset (you'll find all the info that you need in Kaggle). \n",
    "\n",
    "# IMPORTS\n",
    "    # Libraries and packages such as Pandas and Numpy are already imported above, in the first cell of this notebook.\n",
    "\n",
    "# IMPORT DATA SET\n",
    "data = pd.read_csv('titanic.csv')\n",
    "    # 1) Visualize and understand data set, in order to drop no needed columns.\n",
    "data.head()\n",
    "data.describe()\n",
    "data.drop('Name', axis=1, inplace=True) # I will not use Name as an input variable\n",
    "data['Sex'] = data['Sex'].map({'male': 0, 'female': 1}) # Change strings to floats in the 'sex' column.\n",
    "    # I could have done the 'sex' column transformation with dummy variables, but this works too.\n",
    "    # 2) Check data types are the ones I expect, and change the ones that are not the correct type.\n",
    "data.info() # since all of them are integers and floats, it is fine so far.\n",
    "    # No need to get rid of null values, since according to data.info() there are non-null for each of the columns.\n",
    "    # 3) Fill any missing values with e.g. mean or median - Drop any observations with too many missing features (none missing)\n",
    "    # 4) If your data contains categorical values - inspect their values (no, all numerial now)\n",
    "    # 5) Convert categorical values to 0 and 1 (done that already with 'sex' column)\n",
    "    # 6) Inspect my data again and make sure everything looks good (since I have not changed it since last .info(), no need)\n",
    "data.head() # to see final version\n",
    "# SELECT INPUT AND TARGET/OUTPUT VARIABLES (and transform to numpy array)\n",
    "X = data.drop('Survived', axis=1).values\n",
    "y = data['Survived'].values\n",
    "print(X.shape, y.shape) # check, so I do not get error mistakes later on such as the ones received yesterday.\n",
    "\n",
    "    # Now we standardize our data (so we can replicate the comparison of algorithms and with/without scaling)\n",
    "\n",
    "std_scaler=preprocessing.StandardScaler()\n",
    "X_std=std_scaler.fit_transform(X)\n",
    "\n",
    "minmax_scaler=preprocessing.MinMaxScaler()\n",
    "X_minmax=minmax_scaler.fit_transform(X)\n",
    "\n",
    "    # Create the empty DataFrames for plotting later\n",
    "resall=pd.DataFrame()\n",
    "res_w1=pd.DataFrame()\n",
    "res_w2=pd.DataFrame()\n",
    "res_w3=pd.DataFrame()\n",
    "\n",
    "# ALGORITHMS \n",
    "    # And now I proceed with the different algorithms, applying each one to the three different (scaled)'version' of my variables.\n",
    "    # Copy-pasted this part of the code from this same Mission, part 1. Just made some adjustments in the code.\n",
    "\n",
    "print()\n",
    "print(\"---------------------------------------- LOGISTIC REGRESSION ------------------------------------------------\")\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "# Imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# KFold\n",
    "kfold=KFold(n_splits=10, random_state=7)\n",
    "\n",
    "model=LogisticRegression(solver=\"liblinear\")\n",
    "\n",
    "# Accuracy for X\n",
    "results=cross_val_score(model, X, y, cv=kfold)\n",
    "print(f'Logistic Regression - Accuracy {results.mean()*100:.3f}% std {results.std()*100:3f}')\n",
    "\n",
    "# Accuracy for X_std\n",
    "results_scl=cross_val_score(model, X_std, y, cv=kfold)\n",
    "print(f'Logistic Regression (-1..1) - Accuracy {results_scl.mean()*100:.3f}% std {results_scl.std()*100:3f}')\n",
    "\n",
    "# Accuracy for X_minmax\n",
    "results_minmax=cross_val_score(model, X_minmax, y, cv=kfold)\n",
    "print(f'Logistic Regression ( 0..1) - Accuracy {results_minmax.mean()*100:.3f}% std {results_minmax.std()*100:3f}')\n",
    "\n",
    "\n",
    "res_w1[\"Res\"]=results\n",
    "res_w1[\"Type\"]=\"log\"\n",
    "\n",
    "res_w2[\"Res\"]=results_scl\n",
    "res_w2[\"Type\"]=\"log -1..1\"\n",
    "\n",
    "res_w3[\"Res\"]=results_minmax\n",
    "res_w3[\"Type\"]=\"log 0..1\"\n",
    "\n",
    "resall=pd.concat([resall,res_w1,res_w2,res_w3], ignore_index=True)\n",
    "\n",
    "print()\n",
    "print(\"---------------------------------------- LDA ------------------------------------------------\")\n",
    "\n",
    "# LDA - Linear Discriminant Analysis \n",
    "\n",
    "# Imports (no need for KFold and so on because they are above)\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# KFold\n",
    "kfold=KFold(n_splits=10, random_state=7)\n",
    "\n",
    "model=LinearDiscriminantAnalysis()\n",
    "\n",
    "results=cross_val_score(model, X, y, cv=kfold)\n",
    "\n",
    "print(f'LDA Linear Discriminant Analysis - Accuracy {results.mean()*100:.3f}% std {results.std()*100:3f}')\n",
    "\n",
    "results_scl=cross_val_score(model, X_std, y, cv=kfold)\n",
    "\n",
    "print(f'LDA (-1..1) - Accuracy {results_scl.mean()*100:.3f}% std {results_scl.std()*100:3f}')\n",
    "\n",
    "results_minmax=cross_val_score(model, X_minmax, y, cv=kfold)\n",
    "\n",
    "print(f'LDA ( 0..1) - Accuracy {results_minmax.mean()*100:.3f}% std {results_minmax.std()*100:3f}')\n",
    "\n",
    "\n",
    "res_w1[\"Res\"]=results\n",
    "res_w1[\"Type\"]=\"LDA\"\n",
    "\n",
    "res_w2[\"Res\"]=results_scl\n",
    "res_w2[\"Type\"]=\"LDA -1..1\"\n",
    "\n",
    "res_w3[\"Res\"]=results_minmax\n",
    "res_w3[\"Type\"]=\"LDA 0..1\"\n",
    "\n",
    "resall=pd.concat([resall,res_w1,res_w2,res_w3], ignore_index=True)\n",
    "\n",
    "print()\n",
    "print(\"---------------------------------------- KNN ------------------------------------------------\")\n",
    "# KNN Classification\n",
    "\n",
    "# Imports (no need for KFold and so on because I have imported them above in this same cell)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# KFold\n",
    "kfold=KFold(n_splits=10, random_state=7)\n",
    "model=KNeighborsClassifier()\n",
    "\n",
    "\n",
    "results=cross_val_score(model, X, y, cv=kfold)\n",
    "\n",
    "print(f'KNN - Accuracy {results.mean()*100:.3f}% std {results.std()*100:3f}')\n",
    "\n",
    "results_scl=cross_val_score(model, X_std, y, cv=kfold)\n",
    "\n",
    "print(f'KNN (-1..1) - Accuracy {results_scl.mean()*100:.3f}% std {results_scl.std()*100:3f}')\n",
    "\n",
    "results_minmax=cross_val_score(model, X_minmax, y, cv=kfold)\n",
    "\n",
    "print(f'KNN ( 0..1) - Accuracy {results_minmax.mean()*100:.3f}% std {results_minmax.std()*100:3f}')\n",
    "\n",
    "# scaling in knn is necessary ...\n",
    "\n",
    "res_w1[\"Res\"]=results\n",
    "res_w1[\"Type\"]=\"KNN\"\n",
    "\n",
    "res_w2[\"Res\"]=results_scl\n",
    "res_w2[\"Type\"]=\"KNN -1..1\"\n",
    "\n",
    "res_w3[\"Res\"]=results_minmax\n",
    "res_w3[\"Type\"]=\"KNN 0..1\"\n",
    "\n",
    "resall=pd.concat([resall,res_w1,res_w2,res_w3], ignore_index=True)\n",
    "\n",
    "print()\n",
    "print(\"---------------------------------------- NAIVE BAYES ------------------------------------------------\")\n",
    "# Naive Bayes\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "kfold=KFold(n_splits=10, random_state=7)\n",
    "\n",
    "model=GaussianNB()\n",
    "\n",
    "results=cross_val_score(model, X, y, cv=kfold)\n",
    "\n",
    "print(f'Naive Bayes - Accuracy {results.mean()*100:.3f}% std {results.std()*100:3f}')\n",
    "\n",
    "results_scl=cross_val_score(model, X_std, y, cv=kfold)\n",
    "\n",
    "print(f'Naive Bayes (-1..1) - Accuracy {results_scl.mean()*100:.3f}% std {results_scl.std()*100:3f}')\n",
    "\n",
    "results_minmax=cross_val_score(model, X_minmax, y, cv=kfold)\n",
    "\n",
    "print(f'Naive Bayes ( 0..1) - Accuracy {results_minmax.mean()*100:.3f}% std {results_minmax.std()*100:3f}')\n",
    "\n",
    "res_w1[\"Res\"]=results\n",
    "res_w1[\"Type\"]=\"NB\"\n",
    "\n",
    "res_w2[\"Res\"]=results_scl\n",
    "res_w2[\"Type\"]=\"NB -1..1\"\n",
    "\n",
    "res_w3[\"Res\"]=results_minmax\n",
    "res_w3[\"Type\"]=\"NB 0..1\"\n",
    "\n",
    "resall=pd.concat([resall,res_w1,res_w2,res_w3], ignore_index=True)\n",
    "\n",
    "print()\n",
    "print(\"---------------------------------------- DECISION TREE ------------------------------------------------\")\n",
    "\n",
    "# Decision Trees\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "seed=7\n",
    "\n",
    "kfold=KFold(n_splits=10, random_state=seed)\n",
    "\n",
    "model=DecisionTreeClassifier(class_weight=\"balanced\", random_state=seed)\n",
    "\n",
    "\n",
    "results=cross_val_score(model, X, y, cv=kfold)\n",
    "\n",
    "print(f'Decision Tree - Accuracy {results.mean()*100:.3f}% std {results.std()*100:3f}')\n",
    "\n",
    "results_scl=cross_val_score(model, X_std, y, cv=kfold)\n",
    "\n",
    "print(f'Decision Tree (-1..1) - Accuracy {results_scl.mean()*100:.3f}% std {results_scl.std()*100:3f}')\n",
    "\n",
    "results_minmax=cross_val_score(model, X_minmax, y, cv=kfold)\n",
    "\n",
    "print(f'Decision Tree ( 0..1) - Accuracy {results_minmax.mean()*100:.3f}% std {results_minmax.std()*100:3f}')\n",
    "\n",
    "res_w1[\"Res\"]=results\n",
    "res_w1[\"Type\"]=\"DT\"\n",
    "\n",
    "res_w2[\"Res\"]=results_scl\n",
    "res_w2[\"Type\"]=\"DT -1..1\"\n",
    "\n",
    "res_w3[\"Res\"]=results_minmax\n",
    "res_w3[\"Type\"]=\"DT 0..1\"\n",
    "\n",
    "resall=pd.concat([resall,res_w1,res_w2,res_w3], ignore_index=True)\n",
    "\n",
    "\n",
    "# Displaying the interactive tree\n",
    "\n",
    "# Imports\n",
    "from IPython.display import HTML \n",
    "from sklearn import tree\n",
    "from graphviz import Source\n",
    "from IPython.display import SVG, display\n",
    "from ipywidgets import interactive\n",
    "\n",
    "seed=7\n",
    "\n",
    "def plot_tree(crit, split, depth, min_split, min_leaf=1):\n",
    "    \n",
    "    data_tree=DecisionTreeClassifier(random_state=seed,\n",
    "                criterion=crit, \n",
    "                splitter=split,\n",
    "                max_depth=depth,\n",
    "                min_samples_split=min_split,  \n",
    "                min_samples_leaf=min_leaf)\n",
    "    data_tree.fit(X,y)\n",
    "    \n",
    "    graph=Source(tree.export_graphviz(data_tree,\n",
    "            out_file=None,\n",
    "            feature_names=data.columns[1:], # All the columns I have in the preprocessed version of data, except 'Survived'\n",
    "            class_names=[\"0\",\"1\"], # only two classes: survived or not\n",
    "            filled=True,\n",
    "            rounded=True))\n",
    "    display(SVG(graph.pipe(format=\"svg\"))) \n",
    "    \n",
    "    return data_tree\n",
    "\n",
    "inter=interactive(plot_tree,\n",
    "        crit=[\"gini\",\"entropy\"],\n",
    "        split=[\"best\",\"random\"],\n",
    "        depth=[None,1,2,3,4],\n",
    "        min_split=(2,100),\n",
    "        min_leaf=(1,200))\n",
    "\n",
    "display(inter)\n",
    "\n",
    "print()\n",
    "print(\"---------------------------------------- SVM ------------------------------------------------\")\n",
    "\n",
    "# SVM - Support Vector Machines\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "kfold=KFold(n_splits=10, random_state=7)\n",
    "\n",
    "model=SVC(gamma=\"scale\")\n",
    "\n",
    "results=cross_val_score(model, X, y, cv=kfold)\n",
    "\n",
    "print(f'Support Vector Machines - Accuracy {results.mean()*100:.3f}% std {results.std()*100:3f}')\n",
    "\n",
    "results_scl=cross_val_score(model, X_std, y, cv=kfold)\n",
    "\n",
    "print(f'SVM (-1..1) - Accuracy {results_scl.mean()*100:.3f}% std {results_scl.std()*100:3f}')\n",
    "\n",
    "results_minmax=cross_val_score(model, X_minmax, y, cv=kfold)\n",
    "\n",
    "print(f'SVM ( 0..1) - Accuracy {results_minmax.mean()*100:.3f}% std {results_minmax.std()*100:3f}')\n",
    "\n",
    "# the importance of scaling depends on the kernel used\n",
    "\n",
    "res_w1[\"Res\"]=results\n",
    "res_w1[\"Type\"]=\"SVM\"\n",
    "\n",
    "res_w2[\"Res\"]=results_scl\n",
    "res_w2[\"Type\"]=\"SVM -1..1\"\n",
    "\n",
    "res_w3[\"Res\"]=results_minmax\n",
    "res_w3[\"Type\"]=\"SVM 0..1\"\n",
    "\n",
    "resall=pd.concat([resall,res_w1,res_w2,res_w3], ignore_index=True)\n",
    "\n",
    "print()\n",
    "print(\"---------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"---------------------------------------- ALGORITHM COMPARISON ------------------------------------------------\")\n",
    "\n",
    "# Now let's compare them all \n",
    "\n",
    "plt.figure(figsize=(15,9))\n",
    "\n",
    "sns.boxplot(data=resall, x=\"Type\", y=\"Res\")\n",
    "\n",
    "sns.swarmplot(data=resall, x=\"Type\", y=\"Res\", color=\"royalblue\")\n",
    "\n",
    "\n",
    "\n",
    "# INTERPRETATION: (in next cell:)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to choose an algorithm, there are two main factors that need to be taken into account:\n",
    "- Average of the accuracy (as high as possible, the line in the middle of the boxplot/whisker diagram)\n",
    "- Low variance (which can be seen with the range in the box, from the 25%-75% percentile, and the lines outside for the rest, as well as the outliers).\n",
    "\n",
    "Therefore, I discard KNN and SVM without standardization.\n",
    "Although KNN minmax is the highest in average accuracy, the variance is great as well, and therefore would not opt for it.\n",
    "My first options would be SVM with the features standardized (either with MinMax or the StandardScaler).\n",
    "Second option would be Naive Bayes, and third LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
